# IMEDIA Project

> **Estado**: MVP funcional (Reddit) Â· Arquitectura escalable a Facebook/Threads/X Â· Medallion (`raw â†’ bronze â†’ silver â†’ gold`)

---

## 1) IntroducciÃ³n
En un entorno donde la informaciÃ³n cambia a velocidad absurda, **IMEDIA Project** nace para ayudarte a **explorar, analizar y entender** lo que pasa en redes sociales y comunidades online. Empezamos por **Reddit** (API abierta, estable), pero la arquitectura ya estÃ¡ pensada para sumar **Facebook, Threads, X (Twitter)** u otras fuentes.

IMEDIA implementa un pipeline de datos reproducible (con **uv**, **Polars**, **SQLite**) que extrae publicaciones/comentarios, los procesa por capas (**medallion**), y deja la puerta abierta a **NLP** y **dashboards**.

---

## 2) Problema y justificaciÃ³n
- Las personas y equipos necesitan **monitorizar tendencias**, **temas** y **sentimientos** sin perderse en scroll infinito.
- Las APIs y formatos cambian; necesitas una **arquitectura modular**, **idempotente** y **trazable**.
- Herramientas low-code/BI ayudan a visualizar, pero el valor nace en **datos limpios** y **modelados**.

**IMEDIA** justifica su existencia al: (1) estandarizar ingestion y almacenamiento, (2) dejar un **track** auditable por capas, (3) facilitar anÃ¡lisis avanzados (NLP, LLMs, dashboards) sobre bases sÃ³lidas.

---

## 3) Objetivos
**General**: Automatizar la **recolecciÃ³n y procesamiento** de contenido social para habilitar anÃ¡lisis y visualizaciones confiables.

**EspecÃ­ficos**:
- Ingerir contenido de Reddit vÃ­a API oficial (PRAW).
- Normalizar y persistir en formato analÃ­tico (**Parquet**) y en una base **SQLite** para consultas rÃ¡pidas.
- Preparar **dimensiones** y **hechos** (SILVER) que soporten KPIs y anÃ¡lisis posteriores.
- DiseÃ±ar una CLI reproducible con **uv** para orquestar corridas.
- Roadmap: sentiment/topic modeling, features para LLMs, dashboards (Power BI/DuckDB, etc.).

---

## 4) Alcance (MVP) y estado actual
- **Fuente**: Reddit âœ…
- **Pipeline**: `raw â†’ bronze â†’ silver` âœ… (gold en diseÃ±o)
- **Persistencia**: Parquet + SQLite âœ…
- **CLI**: modo *subreddit Ãºnico* y modo *descubrimiento de N subreddits â€œhotâ€* âœ…
- **Comentarios**: descarga del **primer post** por subreddit (flag `--fetch-comments`) âœ…
- **GOLD**: KPIs/ML/LLMs (pendiente) ğŸ”œ

---

## 5) Arquitectura del pipeline (Medallion)
```
Reddit API â†’ [RAW] NDJSON (as-is) â†’ [BRONZE] Parquet (tipado/flatten) â†’ [SILVER] Parquet+SQLite (dims/facts) â†’ [GOLD] KPIs/Features/LLMs
```
**Capas**:
- **RAW**: dumps sin transformar (NDJSON por origen/lote). No se borra ni se pisa.
- **BRONZE**: tipado suave, separar campos, **sin perder columnas** (nulos permitidos). Particionado por fecha (`created_utc`).
- **SILVER**: normalizaciÃ³n (dimensiones y tablas de hechos), claves coherentes, **upsert** en SQLite.
- **GOLD**: mÃ©tricas, agregados, features ML y vistas para BI (en construcciÃ³n).

**Modelado (SILVER)**:
- `dim_subreddit(subreddit, subscribers, description, created_utc, over18)`
- `dim_author(author_name)`
- `fact_posts(post_id, subreddit, author, title, selftext, url, score, num_comments, over_18, created_utc, â€¦)`
- `fact_comments(comment_id, post_id, author, body, created_utc, score, â€¦)`

---

## 6) Estructura del repositorio
```
imedia/
â”œâ”€ pyproject.toml
â”œâ”€ .env.example
â”œâ”€ README.md
â”œâ”€ db/
â”‚  â””â”€ imedia.sqlite
â””â”€ data/
   â”œâ”€ raw/reddit/
   â”‚  â”œâ”€ posts/part-<batch>-[subreddit].ndjson
   â”‚  â”œâ”€ comments/part-<batch>-[post_id].ndjson
   â”‚  â”œâ”€ subreddits/part-<batch>-[subreddit].ndjson
   â”‚  â””â”€ hot_sublists/part-<batch>-<estrategia>.ndjson
   â”œâ”€ bronze/reddit/
   â”‚  â”œâ”€ posts/YYY=â€¦/MM=â€¦/DD=â€¦/posts__<batch>__<subreddit>.parquet
   â”‚  â”œâ”€ comments/YYY=â€¦/MM=â€¦/DD=â€¦/comments__<batch>__<post_id>.parquet
   â”‚  â”œâ”€ subreddits/subreddits-<batch>-<subreddit>.parquet
   â”‚  â””â”€ hot_sublists/hot_sublists-<batch>-<uid>.parquet
   â””â”€ silver/reddit/
      â”œâ”€ dim_subreddit.parquet
      â”œâ”€ dim_author.parquet
      â”œâ”€ fact_posts.parquet
      â””â”€ fact_comments.parquet
```

### CÃ³digo fuente (src/)
- `config.py` â€” rutas, batch id, env vars
- `reddit_client.py` â€” autenticaciÃ³n PRAW (read-only)
- `raw_extractor.py` â€” descarga **as-is** a RAW
- `bronze_transformer.py` â€” tipado/flatten â†’ BRONZE
- `silver_normalizer.py` â€” normalizaciÃ³n + upsert a SQLite
- `gold_products.py` â€” placeholder para KPIs/ML
- `repo_sqlite.py` â€” DDL + upserts
- `utils.py` â€” helpers (slugify, casts robustos)
- `__main__.py` â€” CLI orquestador

---

## 7) Requisitos
- **Python** â‰¥ 3.11
- **uv** (gestiÃ³n de entornos ultra-rÃ¡pida) â†’ https://docs.astral.sh/uv/
- ConexiÃ³n a internet (para API Reddit)

---

## 8) InstalaciÃ³n
```bash
# 1) clona el repo
git clone <URL-del-repo>
cd imedia

# 2) instala dependencias
uv sync

# 3) copia variables de entorno
type .env.example > .env   # (Windows: cp .env.example .env)
```

### Variables de entorno (`.env`)
```env
REDDIT_CLIENT_ID=tu_client_id
REDDIT_CLIENT_SECRET=tu_client_secret
REDDIT_USER_AGENT=imedia/0.1 by <tu_usuario>
IMEDIA_DB_PATH=db/imedia.sqlite
IMEDIA_DATA_ROOT=data
REDDIT_REQUEST_TIMEOUT=30
# opcional para etiquetar corridas manualmente
# IMEDIA_BATCH_TS=20250101_1200
```

### Test de autenticaciÃ³n
```bash
uv run python -c "from imedia.reddit_client import RedditClient; r=RedditClient().reddit; s=r.subreddit('python'); print('OK Reddit! subs:', getattr(s,'subscribers',None))"
```

---

## 9) Uso (CLI)
La CLI vive en `__main__.py`. Ejecuta con `uv run python -m imedia [opciones]`.

### Modos (exclusivos)
1. **Subreddit Ãºnico**
```bash
uv run python -m imedia \
  --subreddit python \
  --limit 50 \
  --time-filter day \
  --fetch-comments
```
2. **Descubrir N subreddits â€œhotâ€** (y descargar posts de cada uno)
```bash
uv run python -m imedia \
  --discover-hot 10 \
  --hot-strategy all_top_day \
  --limit 30 \
  --include-nsfw    # opcional
```

### ParÃ¡metros
| ParÃ¡metro | Tipo | Obligatorio | Default | DescripciÃ³n |
|---|---:|:---:|---:|---|
| `--subreddit <nombre>` | str | **Mutuamente excluyente** con `--discover-hot` | â€” | Modo 1: ingestiÃ³n de un subreddit especÃ­fico. |
| `--discover-hot <N>` | int | **Mutuamente excluyente** con `--subreddit` | â€” | Modo 2: descubre N subreddits â€œcalientesâ€ y descarga posts de cada uno. |
| `--hot-strategy {popular,all_hot,all_top_day}` | str | No (solo aplica con `--discover-hot`) | `popular` | CÃ³mo descubrir subreddits: `popular` (rÃ¡pido), `all_hot` (zeitgeist), `all_top_day` (mejores del dÃ­a). |
| `--include-nsfw` | flag | No | `false` | Incluir subreddits NSFW en el descubrimiento. |
| `--limit <N>` | int | No | `100` | Posts a descargar **por subreddit**. |
| `--time-filter {hour,day,week,month,year,all}` | str | No | `day` | Ventana temporal para `top`. |
| `--fetch-comments` | flag | No | `false` | Descarga comentarios del **primer post** en cada subreddit del lote. |

> **Nota**: `--fetch-comments` actualmente trae **solo** el primer post de cada subreddit. Un flag `--all-comments` puede aÃ±adirse en el roadmap.

### Ejemplos Ãºtiles
- Top 20 `machinelearning` Ãºltima semana con comentarios del primer post:
```bash
uv run python -m imedia --subreddit machinelearning --limit 20 --time-filter week --fetch-comments
```
- Descubrir 15 subreddits por popularidad e ingerir 40 posts por cada uno:
```bash
uv run python -m imedia --discover-hot 15 --hot-strategy popular --limit 40
```

---

## 10) Salidas esperadas
- **RAW**: NDJSON por origen (no se pisa). Ej: `data/raw/reddit/posts/part-<batch>-python.ndjson`.
- **BRONZE**: Parquet particionado por `YYYY/MM/DD` (posts/comments) + archivos Ãºnicos por sub/post.
- **SILVER**: `dim_*.parquet`, `fact_*.parquet` y **SQLite** poblado (`db/imedia.sqlite`).

---

## 11) VerificaciÃ³n rÃ¡pida (post-run)
```bash
# conteos en SQLite
uv run python - <<'PY'
import sqlite3
con = sqlite3.connect('db/imedia.sqlite')
for t in ('subreddits','authors','posts','comments'):
    try:
        n = con.execute(f'SELECT count(*) FROM {t}').fetchone()[0]
        print(t, n)
    except Exception as e:
        print(t, 'no existe:', e)
PY
```
```bash
# inspeccionar SILVER
uv run python - <<'PY'
import polars as pl
p = pl.read_parquet('data/silver/reddit/fact_posts.parquet')
print('subs distintos:', p.select('subreddit').n_unique())
print('total posts:', p.height)
print(p.select('subreddit').unique().head(15))
PY
```

---

## 12) SoluciÃ³n de problemas comunes
- **`ValueError: Faltan variables en .env`** â†’ Completa `REDDIT_CLIENT_ID/SECRET/USER_AGENT`.
- **`OAuthException`** (PRAW) â†’ Verifica que tu app de Reddit sea de tipo **script** y que el secret sea correcto.
- **Timeouts** â†’ Aumenta `REDDIT_REQUEST_TIMEOUT` (ej. 60) o reduce `--limit` y la cantidad de subreddits.

---

## 13) Roadmap (sujeto a cambios)
- `--all-comments` (comentarios de todos los posts del lote)
- Capa **GOLD**: KPIs (7d/24h), engagement por hora, features para modelos
- IntegraciÃ³n **LLMs**: Q&A sobre corpus, resÃºmenes temÃ¡ticos
- MÃ¡s fuentes: X/Threads/Facebook (cuando polÃ­ticas y APIs lo permitan)
- Export a **DuckDB/ADBC** y/o formatos **Delta/Iceberg** para datasets grandes
- Tests `pytest` + `ruff` 

---

### CrÃ©ditos
- **PRAW**, **Polars**, **uv** y comunidad OSS â¤ï¸

